{"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{},"source":["# ST446 Distributed Computing for Big Data\n","\n","\n","## Week 4 class: Spark SQL and DataFrames\n","\n","\n","### LT 2021\n","\n","This notebook contains code samples from Chapter 4 of the book Learning PySpark."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Generate a DataFrame \n","\n","### a. Data from file\n","We again use the `author-large.txt` file from dblp, which we used in previous exercises. We load the data into a DataFrame using the function `spark.read.csv()`.\n","\n","(Please create a folder called `flight-data` in your bucket and copy the files under `week03/class/flight-data/` into your bucket. And the result should be like\n","\n","```\n","(base) LSE021353-2:~ st446$ gsutil ls gs://jialin-bucket/flight-data\n","gs://jialin-bucket/flight-data/airport-codes-na.txt\n","gs://jialin-bucket/flight-data/departuredelays.csv\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["filename = 'gs://jialin-bucket/data/author-large.txt'\n","\n","author_large_no_schema = spark.read.csv(filename, \n","                    header='false', sep='\\t')\n","author_large_no_schema.createOrReplaceTempView(\"author_large_no_schema\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Inferring the Schema Using Reflection\n","Spark can infer the schema using _reflection_; i.e. automaticaly determining the schema of the data based on sampling the data."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n\n"}],"source":["author_large_no_schema.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["#### Programmatically Specifying the Schema\n","Spark also allows to programmatically specify the schema."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import *\n","\n","schema = StructType([\n","    StructField(\"author\", StringType(), True),    \n","    StructField(\"journal\", StringType(), True),\n","    StructField(\"title\", StringType(), True),\n","    StructField(\"year\", LongType(), True)\n","])\n","\n","author_large = spark.read.csv(filename, \n","                    header='false', schema=schema, sep='\\t')\n","author_large.createOrReplaceTempView(\"author_large\")"]},{"cell_type":"code","execution_count":4,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":"root\n |-- author: string (nullable = true)\n |-- journal: string (nullable = true)\n |-- title: string (nullable = true)\n |-- year: long (nullable = true)\n\n"}],"source":["author_large.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark to infer the schema via reflection.\n","\n","Additional resources:\n","* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n","* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema) (for programmatically specifying the schema from a `csv` file)."]},{"cell_type":"markdown","metadata":{},"source":["#### SparkSession\n","\n","Notice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`, which provides:\n","* entry point for reading data,\n","* working with metadata,\n","* configuration,\n","* cluster resource management.\n","\n","For more information, see [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1)."]},{"cell_type":"markdown","metadata":{},"source":["### b. Generate your own DataFrame\n","Instead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|  Katie|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"}],"source":["# Generate our own JSON data \n","#   This way we don't have to access the file system yet.\n","stringJSONRDD = sc.parallelize((\"\"\" \n","  { \"id\": \"123\",\n","    \"name\": \"Katie\",\n","    \"age\": 19,\n","    \"eyeColor\": \"brown\"\n","  }\"\"\",\n","   \"\"\"{\n","    \"id\": \"234\",\n","    \"name\": \"Michael\",\n","    \"age\": 22,\n","    \"eyeColor\": \"green\"\n","  }\"\"\", \n","  \"\"\"{\n","    \"id\": \"345\",\n","    \"name\": \"Simone\",\n","    \"age\": 23,\n","    \"eyeColor\": \"blue\"\n","  }\"\"\")\n",") \n","# Create DataFrame\n","swimmersJSON = spark.read.json(stringJSONRDD) \n","# Create temporary table\n","swimmersJSON.createOrReplaceTempView(\"swimmersJSON\") \n","# DataFrame API\n","swimmersJSON.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Querying with the DataFrame API\n","\n","Spark allows you to query a DataFrame using the DataFrame API."]},{"cell_type":"markdown","metadata":{},"source":["### a. Show first few rows"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+--------------------+--------------------+----+\n|              author|             journal|               title|year|\n+--------------------+--------------------+--------------------+----+\n|   Jurgen Annevelink|Modern Database S...|Object SQL - A La...|1995|\n|         Rafiul Ahad|Modern Database S...|Object SQL - A La...|1995|\n|      Amelia Carlson|Modern Database S...|Object SQL - A La...|1995|\n|   Daniel H. Fishman|Modern Database S...|Object SQL - A La...|1995|\n|  Michael L. Heytens|Modern Database S...|Object SQL - A La...|1995|\n|        William Kent|Modern Database S...|Object SQL - A La...|1995|\n|     Jos A. Blakeley|Modern Database S...|OQL[C++]: Extendi...|1995|\n|      Yuri Breitbart|Modern Database S...|Transaction Manag...|1995|\n|Hector Garcia-Molina|Modern Database S...|Transaction Manag...|1995|\n|Abraham Silberschatz|Modern Database S...|Transaction Manag...|1995|\n+--------------------+--------------------+--------------------+----+\nonly showing top 10 rows\n\n"}],"source":["author_large.show(10)"]},{"cell_type":"markdown","metadata":{},"source":["### b. Count rows"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":"2225370"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["author_large.count()"]},{"cell_type":"markdown","metadata":{},"source":["### c. Filter \n","\n","Here we get the titles with year 2000:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+\n|               title|\n+--------------------+\n|Leveraging Relati...|\n|Faster FFTs via A...|\n|From Causal Theor...|\n|Team-Aware Multir...|\n|Key Management fo...|\n|An Algorithm for ...|\n|Belief Revision P...|\n|ATM network manag...|\n|Building Virtual ...|\n|A Modular Approac...|\n+--------------------+\nonly showing top 10 rows\n\n"}],"source":["author_large.select(author_large.title).distinct().filter(author_large.year == 2000).show(10)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+\n|               title|\n+--------------------+\n|Leveraging Relati...|\n|Faster FFTs via A...|\n|From Causal Theor...|\n|Team-Aware Multir...|\n|Key Management fo...|\n|An Algorithm for ...|\n|Belief Revision P...|\n|ATM network manag...|\n|Building Virtual ...|\n|A Modular Approac...|\n+--------------------+\nonly showing top 10 rows\n\n"}],"source":["author_large.select(\"title\").distinct().filter(\"year = 2000\").show(10)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Querying with Spark SQL\n","You can also write your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL). The following codes produces the same output that in the section \"Querying with the DataFrame API\"."]},{"cell_type":"markdown","metadata":{},"source":["### a. Show first few rows"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+--------------------+--------------------+----+\n|              author|             journal|               title|year|\n+--------------------+--------------------+--------------------+----+\n|   Jurgen Annevelink|Modern Database S...|Object SQL - A La...|1995|\n|         Rafiul Ahad|Modern Database S...|Object SQL - A La...|1995|\n|      Amelia Carlson|Modern Database S...|Object SQL - A La...|1995|\n|   Daniel H. Fishman|Modern Database S...|Object SQL - A La...|1995|\n|  Michael L. Heytens|Modern Database S...|Object SQL - A La...|1995|\n|        William Kent|Modern Database S...|Object SQL - A La...|1995|\n|     Jos A. Blakeley|Modern Database S...|OQL[C++]: Extendi...|1995|\n|      Yuri Breitbart|Modern Database S...|Transaction Manag...|1995|\n|Hector Garcia-Molina|Modern Database S...|Transaction Manag...|1995|\n|Abraham Silberschatz|Modern Database S...|Transaction Manag...|1995|\n+--------------------+--------------------+--------------------+----+\n\n"}],"source":["spark.sql(\"select * from author_large limit 10 \").show()"]},{"cell_type":"markdown","metadata":{},"source":["### b. Count rows"]},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+\n|count(1)|\n+--------+\n| 2225370|\n+--------+\n\n"}],"source":[" spark.sql(\"select count(1) from author_large\").show()"]},{"cell_type":"markdown","metadata":{},"source":["### c. Filter \n","\n","Here we get the titles with year 2000:"]},{"cell_type":"code","execution_count":12,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+\n|               title|\n+--------------------+\n|Leveraging Relati...|\n|Faster FFTs via A...|\n|From Causal Theor...|\n|Team-Aware Multir...|\n|Key Management fo...|\n|An Algorithm for ...|\n|Belief Revision P...|\n|ATM network manag...|\n|Building Virtual ...|\n|A BIST methodolog...|\n+--------------------+\n\n"}],"source":["spark.sql(\"select DISTINCT title from author_large where year = 2000 limit 10\").show()"]},{"cell_type":"markdown","metadata":{},"source":["Here we query publications with title starting with letter `b`: "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+\n|               title|\n+--------------------+\n|      buyer's agent.|\n|      buyer's agent.|\n|bicACO: An Ant Co...|\n|bicACO: An Ant Co...|\n|bicACO: An Ant Co...|\n|            bottom).|\n|ber den Abstrakti...|\n|berlegungen beim ...|\n|ber die Realisier...|\n|berlegungen zu ei...|\n+--------------------+\n\n"}],"source":["spark.sql(\"select title from author_large where title like 'b%' limit 10\").show()"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Query by joining 2 tables"]},{"cell_type":"markdown","metadata":{},"source":["Let's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":"DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Set File Paths\n","flightPerfFilePath = \"gs://jialin-bucket/flight-data/departuredelays.csv\"\n","airportsFilePath = \"gs://jialin-bucket/flight-data/airport-codes-na.txt\"\n","\n","# Obtain Airports dataset\n","airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n","airports.createOrReplaceTempView(\"airports\")\n","\n","# Obtain Departure Delays dataset\n","flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n","flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n","\n","# Cache the Departure Delays dataset \n","flightPerf.cache()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+----------+-----+-------+----+\n|      City|State|Country|IATA|\n+----------+-----+-------+----+\n|Abbotsford|   BC| Canada| YXX|\n|  Aberdeen|   SD|    USA| ABR|\n|   Abilene|   TX|    USA| ABI|\n|     Akron|   OH|    USA| CAK|\n|   Alamosa|   CO|    USA| ALS|\n+----------+-----+-------+----+\nonly showing top 5 rows\n\n+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n|01031245|   -4|     602|   ABE|        ATL|\n+--------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n"}],"source":["airports.show(5)\n","flightPerf.show(5)"]},{"cell_type":"markdown","metadata":{},"source":["Now we query flight departure delays by cities in WA by joining the performance and airport tables with the airport codes (to identify state and city):"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+------+--------+\n|   City|origin|  Delays|\n+-------+------+--------+\n|Seattle|   SEA|159086.0|\n|Spokane|   GEG| 12404.0|\n|  Pasco|   PSC|   949.0|\n+-------+------+--------+\n\n"}],"source":["# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n","spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"]},{"cell_type":"markdown","metadata":{},"source":["Here we query flight departure delays by States in the US:"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+---------+\n|State|   Delays|\n+-----+---------+\n|   SC|  80666.0|\n|   AZ| 401793.0|\n|   LA| 199136.0|\n|   MN| 256811.0|\n|   NJ| 452791.0|\n|   OR| 109333.0|\n|   VA|  98016.0|\n| null| 397237.0|\n|   RI|  30760.0|\n|   WY|  15365.0|\n|   KY|  61156.0|\n|   NH|  20474.0|\n|   MI| 366486.0|\n|   NV| 474208.0|\n|   WI| 152311.0|\n|   ID|  22932.0|\n|   CA|1891919.0|\n|   CT|  54662.0|\n|   NE|  59376.0|\n|   MT|  19271.0|\n+-----+---------+\nonly showing top 20 rows\n\n"}],"source":["# Query Sum of Flight Delays by State (for the US)\n","spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"]},{"cell_type":"markdown","metadata":{},"source":["For more information, please refer to:\n","* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n","* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n","* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"]}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"PySpark","name":"pyspark","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"2.7.14","name":"python","file_extension":".py","pygments_lexer":"ipython2","codemirror_mode":{"version":2,"name":"ipython"}},"anaconda-cloud":{},"notebookId":4341522646494009,"name":"Ch4 - DataFrames"}}
