{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "\n",
    "\n",
    "## Week 4 class: Spark SQL and DataFrames\n",
    "\n",
    "\n",
    "### LT 2021\n",
    "\n",
    "This notebook contains code samples from Chapter 4 of the book Learning PySpark.\n",
    "\n",
    "You should open a Jupyter Notebook in your Dataproc cluster and create a PySpark notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate a DataFrame \n",
    "\n",
    "### a. Data from file\n",
    "We again use the `author-large.txt` file from dblp, which we used in previous exercises. We load the data into a DataFrame using the function `spark.read.csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to change the command to use your bucket name\n",
    "\n",
    "filename = 'gs://jialin-bucket/data/author-large.txt'\n",
    "\n",
    "author_large_no_schema = spark.read.csv(filename, \n",
    "                    header='false', sep='\\t')\n",
    "author_large_no_schema.createOrReplaceTempView(\"author_large_no_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection\n",
    "Spark can infer the schema using _reflection_; i.e. automaticaly determining the schema of the data based on sampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_large_no_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n",
    "Spark also allows to programmatically specify the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"author\", StringType(), True),    \n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"year\", LongType(), True)\n",
    "])\n",
    "\n",
    "author_large = spark.read.csv(filename, \n",
    "                    header='false', schema=schema, sep='\\t')\n",
    "author_large.createOrReplaceTempView(\"author_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_large.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark to infer the schema via reflection.\n",
    "\n",
    "Additional resources:\n",
    "* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n",
    "* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema) (for programmatically specifying the schema from a `csv` file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparkSession\n",
    "\n",
    "Notice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`, which provides:\n",
    "* entry point for reading data,\n",
    "* working with metadata,\n",
    "* configuration,\n",
    "* cluster resource management.\n",
    "\n",
    "For more information, see [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Generate your own DataFrame\n",
    "Instead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our own JSON data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ") \n",
    "# Create DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD) \n",
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\") \n",
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Querying with the DataFrame API\n",
    "\n",
    "Spark allows you to query a DataFrame using the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_large.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Count rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_large.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Filter \n",
    "\n",
    "Here we get the titles with year 2000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_large.select(author_large.title).distinct().filter(author_large.year == 2000).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_large.select(\"title\").distinct().filter(\"year = 2000\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Querying with Spark SQL\n",
    "You can also write your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL). The following codes produces the same output that in the section \"Querying with the DataFrame API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from author_large limit 10 \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Count rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " spark.sql(\"select count(1) from author_large\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Filter \n",
    "\n",
    "Here we get the titles with year 2000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select DISTINCT title from author_large where year = 2000 limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we query publications with title starting with letter `b`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select title from author_large where title like 'b%' limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query by joining 2 tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a flight performance using DataFrames.\n",
    "\n",
    "Please, follow these steps:\n",
    "\n",
    "1) create a folder called `flight-data` in your bucket.\n",
    "\n",
    "2) copy the files below into your bucket (remember to change the command to use your bucket name). \n",
    "\n",
    "```\n",
    "curl -L -o airport-codes-na.txt https://www.dropbox.com/sh/89xbpcjl\n",
    "4oq0j4w/AACFbu4e8rkwGIIasVBKI08va/LearningPySpark/airport-codes-na.txt?dl=0\n",
    "\n",
    "curl -L -o departuredelays.csv https://www.dropbox.com/sh/89xbpcjl4\n",
    "oq0j4w/AADCRWn4tOKcp21WBPYtcFyna/LearningPySpark/departuredelays.csv?dl=0\n",
    "```\n",
    "\n",
    "3) And the result should be like (remember to change the command to use your bucket name):\n",
    "\n",
    "```\n",
    "(base) LSE021353-2:~ st446$ gsutil ls gs://jialin-bucket/flight-data\n",
    "gs://jialin-bucket/flight-data/airport-codes-na.txt\n",
    "gs://jialin-bucket/flight-data/departuredelays.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first build the DataFrames from the source datasets.\n",
    "# Remember to change the command to use your bucket name\n",
    "\n",
    "# Set File Paths\n",
    "flightPerfFilePath = \"gs://jialin-bucket/flight-data/departuredelays.csv\"\n",
    "airportsFilePath = \"gs://jialin-bucket/flight-data/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# Cache the Departure Delays dataset \n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.show(5)\n",
    "flightPerf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we query flight departure delays by cities in WA by joining the performance and airport tables with the airport codes (to identify state and city):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we query flight departure delays by States in the US:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Sum of Flight Delays by State (for the US)\n",
    "spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, please refer to:\n",
    "* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n",
    "* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "name": "Ch4 - DataFrames",
  "notebookId": 4341522646494009
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
