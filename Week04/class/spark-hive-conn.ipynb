{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "\n",
    "## Week 4 class: Spark SQL and Hive integration\n",
    "\n",
    "### LT 2021\n",
    "\n",
    "In this exercise, we will connect Spark with a local Hive installation that uses MySQL metastore. We will demonstrate that the same tables can be accessed and manipulated from both Hive and Spark. We can persist tables and query them using either Hive or Spark.\n",
    "\n",
    "## 0. Preparation\n",
    "\n",
    "Beforing running this Jupyter notebook, make sure you completed all the commands in the previous three tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|        dblp|\n",
      "|     default|\n",
      "|      fbflow|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show() # your output may differ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create and remove tables using Spark SQL\n",
    "\n",
    "#### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table mytable (key string, val string)\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|  mytable|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show() # your output may differ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the Hive shell by opening a terminal on the VM and typing `hive`. Leave the Hive shell via `ctrl-d`.\n",
    "Check that you see the same from the Hive shell:\n",
    "``` \n",
    "hive> use default;\n",
    "OK\n",
    "Time taken: 0.028 seconds\n",
    "hive> show tables;\n",
    "OK\n",
    "mytable\n",
    "word_count\n",
    "Time taken: 0.031 seconds, Fetched: 2 row(s)\n",
    "hive>\n",
    "```\n",
    "\n",
    "You should see *mytable* when you execute the command `show tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     key|   string|   null|\n",
      "|     val|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe mytable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove\n",
    "Now, remove the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table mytable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check from the Hive shell that the table has been removed:\n",
    "```\n",
    "hive> show tables;\n",
    "OK\n",
    "Time taken: 0.604 seconds\n",
    "hive>\n",
    "```\n",
    "You should see that mytable no longer exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running a query script\n",
    "\n",
    "Upload `Week04/class/test-query.sql` and `Week04/class/people.json` files into a folder named `spark-hive` in your bucket (e.g. `gs://jialin-bucket/spark-hive/`).\n",
    "\n",
    "Then, copy both filhes from your bucket to your cluster VM.\n",
    "\n",
    "```\n",
    "gsutil cp gs://jialin-bucket/spark-hive/test-query.sql .\n",
    "```\n",
    "and\n",
    "```\n",
    "gsutil cp gs://jialin-bucket/spark-hive/people.json .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n",
      "\n",
      "DROP TABLE IF EXISTS mytable\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS mytable (key STRING, val STRING)\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "\n",
      "SHOW TABLES\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|  mytable|      false|\n",
      "+--------+---------+-----------+\n",
      "\n",
      "\n",
      "DESCRIBE mytable\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     key|   string|   null|\n",
      "|     val|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "\n",
      "DROP TABLE mytable\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "\n",
      "SHOW TABLES\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL SparkSession.sql() does not support running multiple query statements. \n",
    "# A way around this is to write a script file and run each statement separately:\n",
    "\n",
    "with open(\"test-query.sql\") as fr:\n",
    "   query = fr.read()\n",
    "\n",
    "for q in query.split(\";\")[:-1]:\n",
    "    print(q) \n",
    "    result = spark.sql(q).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and save a table using `spark.read.json` and `.write.saveAsTable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to change the name to reflect your bucket\n",
    "df = spark.read.json('gs://jialin-bucket/spark-hive/people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|   people|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     age|   bigint|   null|\n",
      "|    name|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete it again...\n",
    "spark.sql(\"drop table people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
