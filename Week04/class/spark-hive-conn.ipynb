{"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":["# ST446 Distributed Computing for Big Data\n","\n","## Week 4 class: Spark SQL and Hive integration\n","\n","### LT 2020\n","\n","In this exercise, we will connect Spark with a local Hive installation that uses mysql metastore. We will demonstrate that the same tables can be accessed and manipulated from both Hive and Spark. We can persist tables and query them using either Hive or Spark.\n","\n","## 0. Preparation\n","\n","Beforing running this jupyter notebook, make sure you completed all the commands in the previous three tutorials."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+------------+\n|databaseName|\n+------------+\n|        dblp|\n|     default|\n|      fbflow|\n+------------+\n\n"}],"source":["spark.sql(\"show databases\").show() # your output may differ!"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Create and remove tables using Spark SQL\n","\n","#### Create"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":"[]"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["spark.sql(\"create table mytable (key string, val string)\").collect()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n| default|  mytable|      false|\n+--------+---------+-----------+\n\n"}],"source":["spark.sql(\"show tables\").show() # your output may differ!"]},{"cell_type":"markdown","metadata":{},"source":["You can access the hive shell by opening a terminal on the VM and typing `hive`. Leave the hive shell via `ctrl-d`.\n","Check that you see the same from the Hive shell:\n","``` \n","hive> use default;\n","OK\n","Time taken: 0.028 seconds\n","hive> show tables;\n","OK\n","mytable\n","word_count\n","Time taken: 0.031 seconds, Fetched: 2 row(s)\n","hive>\n","```\n","\n","You should see mytable when you execute the command `show tables`."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|     key|   string|   null|\n|     val|   string|   null|\n+--------+---------+-------+\n\n"}],"source":["spark.sql(\"describe mytable\").show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Remove\n","Now, remove the table:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"DataFrame[]"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["spark.sql(\"drop table mytable\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"}],"source":["spark.sql(\"show tables\").show()"]},{"cell_type":"markdown","metadata":{},"source":["Check from the Hive shell that the table has been removed:\n","```\n","hive> show tables;\n","OK\n","Time taken: 0.604 seconds\n","hive>\n","```\n","You should see that mytable no longer exists."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Running a query script\n","\n","Upload `week04/class/test-query.sql` and `week04/class/people.json` files into a folder under bucket, here called `gs://jialin-bucket/spark-hive/`.\n","\n","Spark SQL SparkSession.sql() does not support running multiple query statements. A way around this is to write a script file and run each statement separately:"]},{"cell_type":"markdown","metadata":{},"source":["copy two files to your cluster\n","```\n","gsutil cp gs://jialin-bucket/spark-hive/test-query.sql .\n","```\n","and\n","```\n","gsutil cp gs://jialin-bucket/spark-hive/people.json .\n","```"]},{"cell_type":"code","execution_count":8,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":"SHOW TABLES\n+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n\nDROP TABLE IF EXISTS mytable\n++\n||\n++\n++\n\n\nCREATE TABLE IF NOT EXISTS mytable (key STRING, val STRING)\n++\n||\n++\n++\n\n\nSHOW TABLES\n+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n| default|  mytable|      false|\n+--------+---------+-----------+\n\n\nDESCRIBE mytable\n+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|     key|   string|   null|\n|     val|   string|   null|\n+--------+---------+-------+\n\n\nDROP TABLE mytable\n++\n||\n++\n++\n\n\nSHOW TABLES\n+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"}],"source":["with open(\"test-query.sql\") as fr:\n","   query = fr.read()\n","\n","for q in query.split(\";\")[:-1]:\n","    print(q) \n","    result = spark.sql(q).show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Create and save a table using `spark.read.json` and `.write.saveAsTable`"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["df = spark.read.json('gs://jialin-bucket/spark-hive/people.json')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df.write.saveAsTable(\"people\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n| default|   people|      false|\n+--------+---------+-----------+\n\n"}],"source":["spark.sql(\"show tables\").show()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|     age|   bigint|   null|\n|    name|   string|   null|\n+--------+---------+-------+\n\n"}],"source":["spark.sql(\"describe people\").show()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":"DataFrame[]"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# delete it again...\n","spark.sql(\"drop table people\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"}],"source":["spark.sql(\"show tables\").show()"]}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"PySpark","name":"pyspark","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"2.7.14","name":"python","file_extension":".py","pygments_lexer":"ipython2","codemirror_mode":{"version":2,"name":"ipython"}}}}