**ST446 Distributed Computing for Big Data**, LT 2021

# Seminar class 2

This week, we will look at the Hadoop Distributed File System (HDFS) and Google Bigtable.
To this end, we will explore how to run a map-reduce job on the Google Cloud Platform (GCP).

## 0. Preparation (before class)

Before the class, please do the following:
1. Read the preparation part (section 0) in [hadoop_class_activity.md](hadoop_class_activity.md). Verify that you can run hadoop on GCP (or you own computer).
2. Please make sure you can run a jupyter notebook on GCP (see last weeks' homework).
3. Learn how to download GiHub content to your computer and how to copy it into a bucket (you should know this from last week).

## 1. Hadoop File system (in class)

Read the [hadoop_class_activity.md](hadoop_class_activity.md).
In class, we will follow the instructions to explore the file system and run a simple map reduce job.

## 2. Google Bigtable (in class if you have time, or homework)
Google Bigtable is a fully managed NoSQL database service. Here, you will explore how to run Bigtable on GCP from a Jupyter notebook _on your computer_.

1. Follow [google_bigtable_class_activity.ipynb](google_bigtable_class_activity.ipynb) to create your own table.
2. Follow [google_bigtable_class_activity_2.ipynb](google_bigtable_class_activity_2.ipynb) to retrieve an existing table and put your own data into it.
