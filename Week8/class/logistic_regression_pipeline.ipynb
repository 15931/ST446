{"cells":[{"cell_type":"markdown","metadata":{},"source":["# ST446 Distributed Computing for Big Data\n","## Seminar class 7: Scalable machine learning I\n","\n","In this notebook, we run a logistic regression from `spark.ml` on data from the file [voice.csv](voice.csv) from https://www.kaggle.com/primaryobjects/voicegender/data.\n","\n","Please have a look at the Website to understand what data it contains.\n","**Copy it onto your cluster**.\n","\n","Reference: \n","https://github.com/drabastomek/learningPySpark/blob/master/Chapter06/LearningPySpark_Chapter06.ipynb\n","\n","While RDDs and MLlib have already hidden a lot of what's going on behind an API, with DataFrames and ML we can use an even higher-level abstraction, called _ML pipeline_."]},{"cell_type":"markdown","metadata":{},"source":["# 1. Predict the gender of voice with ML"]},{"cell_type":"markdown","metadata":{},"source":["## a. Load and format the data"]},{"cell_type":"markdown","metadata":{},"source":[" I have put the data it into HDFS using `hadoop fs -put voice.csv /tmp/`."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# edit to your bucket location\n","voice_path = \"/tmp/voice.csv\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pyspark.sql.types as typ\n","\n","from pyspark.sql.types import *\n","\n","# metadata / schema for the voice.csv dataset\n","schema = StructType([\n","    StructField(\"meanfreq\", DoubleType(), True),    \n","    StructField(\"sd\", DoubleType(), True),\n","    StructField(\"median\", DoubleType(), True),\n","    StructField(\"Q25\", DoubleType(), True),\n","    StructField(\"Q75\", DoubleType(), True),\n","    StructField(\"IQR\", DoubleType(), True),\n","    StructField(\"skew\", DoubleType(), True),\n","    StructField(\"kurt\", DoubleType(), True),\n","    StructField(\"sp_ent\", DoubleType(), True),\n","    StructField(\"sfm\", DoubleType(), True),\n","    StructField(\"mode\", DoubleType(), True),\n","    StructField(\"centroid\", DoubleType(), True),\n","    StructField(\"meanfun\", DoubleType(), True),\n","    StructField(\"minfun\", DoubleType(), True),\n","    StructField(\"maxfun\", DoubleType(), True),\n","    StructField(\"meandom\", DoubleType(), True),\n","    StructField(\"mindom\", DoubleType(), True),\n","    StructField(\"maxdom\", DoubleType(), True),\n","    StructField(\"dfrange\", DoubleType(), True),\n","    StructField(\"modindx\", DoubleType(), True),\n","    StructField(\"label\", StringType(), True)    \n","])\n","\n","# load the data into a dataframe.\n","voice = spark.read.csv(voice_path, header=True, schema=schema)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+------------------+-----------------+------------------+------------------+------------------+----------------+----------------+-----------------+-----------------+----+------------------+-----------------+------------------+-----------------+---------+---------+---------+-------+-------+-----+\n","|          meanfreq|                sd|           median|               Q25|               Q75|               IQR|            skew|            kurt|           sp_ent|              sfm|mode|          centroid|          meanfun|            minfun|           maxfun|  meandom|   mindom|   maxdom|dfrange|modindx|label|\n","+------------------+------------------+-----------------+------------------+------------------+------------------+----------------+----------------+-----------------+-----------------+----+------------------+-----------------+------------------+-----------------+---------+---------+---------+-------+-------+-----+\n","|0.0597809849598081|0.0642412677031359|0.032026913372582|0.0150714886459209|0.0901934398654331|0.0751219512195122|12.8634618371626|274.402905502067|0.893369416700807|0.491917766397811| 0.0|0.0597809849598081|0.084279106440321|0.0157016683022571|0.275862068965517|0.0078125|0.0078125|0.0078125|    0.0|    0.0| male|\n","+------------------+------------------+-----------------+------------------+------------------+------------------+----------------+----------------+-----------------+-----------------+----+------------------+-----------------+------------------+-----------------+---------+---------+---------+-------+-------+-----+\n","only showing top 1 row\n","\n"]}],"source":["# voice.createGlobalTempView(\"voice\")\n","\n","# dataset structure and sample instance\n","voice.show(1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["3168"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# dataset size\n","voice.count()"]},{"cell_type":"markdown","metadata":{},"source":["### Convert the data into correct format"]},{"cell_type":"markdown","metadata":{},"source":["We need to cast the labels into integer values (instead of doubles) before fitting the model:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- meanfreq: double (nullable = true)\n"," |-- sd: double (nullable = true)\n"," |-- median: double (nullable = true)\n"," |-- Q25: double (nullable = true)\n"," |-- Q75: double (nullable = true)\n"," |-- IQR: double (nullable = true)\n"," |-- skew: double (nullable = true)\n"," |-- kurt: double (nullable = true)\n"," |-- sp_ent: double (nullable = true)\n"," |-- sfm: double (nullable = true)\n"," |-- mode: double (nullable = true)\n"," |-- centroid: double (nullable = true)\n"," |-- meanfun: double (nullable = true)\n"," |-- minfun: double (nullable = true)\n"," |-- maxfun: double (nullable = true)\n"," |-- meandom: double (nullable = true)\n"," |-- mindom: double (nullable = true)\n"," |-- maxdom: double (nullable = true)\n"," |-- dfrange: double (nullable = true)\n"," |-- modindx: double (nullable = true)\n"," |-- label: integer (nullable = true)\n","\n"]}],"source":["voice = voice.withColumn(\"label\", (voice[\"label\"]==\"male\").cast(IntegerType()))\n","voice.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["### Split the data into training and testing samples"]},{"cell_type":"markdown","metadata":{},"source":["We use the `.randomSplit(...)` method to split the data set into a training and test data set."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# split data into training (70% of the samples) and testing (30% of the samples)\n","voice_train, voice_test = voice.randomSplit([0.7, 0.3], seed=1)"]},{"cell_type":"markdown","metadata":{},"source":["## b. Use pipeline to fit the logistic regression\n","\n","A pipeline consists of `Transformers` and `Estimators` to specify an ML workflow:\n","* `Transformer`: an abstraction that includes feature transformers and learned models. A `Transformer` implements a method `transform()`, which converts one DataFrame into another, for example by appending one or more columns.\n","* `Estimator`: abstracts the concept of a learning algorithm or any algorithm that fits or trains a model to/on the data. An `Estimator` implements a method `fit()`, which accepts a DataFrame and produces a model. The model, in turn, is a `Transformer`.\n","* A `Pipeline` is a chain of multiple `Transformers` and `Estimators`.\n","* Generally, `fit()` and `transform()` will be stateless.\n","* A pipeline also has `Parameters`, that can be accessed by the transformer and estimators.\n","\n","One advantage of this paradigm is that we can use the same transformations for training and testing the data. For training, we need to plug in an estimator in the end. For testing, we need to plug in the model that has been produced by the estimator.\n","\n","\n","See [here](https://spark.apache.org/docs/2.2.0/ml-pipeline.html) for more about pipelines.\n","![ml-pipeline](https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/ml-pipeline.png)"]},{"cell_type":"markdown","metadata":{},"source":["### i. Create transformers\n","\n","Create a transformer which creates a single column with all the features collated together.\n","\n","It turns multiple columns into one column containing a vector. Here, this will be the feature vector."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import pyspark.ml.feature as ft\n","\n","featuresCreator = ft.VectorAssembler(\n","    inputCols= voice.columns[:-1],\n","    outputCol='features'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### ii. Create an estimator \n","(we instantiate a logistic regression model with specific parameters)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import pyspark.ml.classification as cl\n","\n","logistic = cl.LogisticRegression(\n","    maxIter=10, \n","    regParam=0.01, \n","    labelCol='label')"]},{"cell_type":"markdown","metadata":{},"source":["### iii. Create a pipeline \n","(create an abstract list of transformers and estimators)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline\n","\n","pipeline = Pipeline(stages=[\n","        featuresCreator, \n","        logistic\n","    ])"]},{"cell_type":"markdown","metadata":{},"source":["### iv. Run the pipeline to fit the model"]},{"cell_type":"markdown","metadata":{},"source":["Now run our `pipeline` and estimate our model."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["model = pipeline.fit(voice_train)\n","test_model = model.transform(voice_test)"]},{"cell_type":"markdown","metadata":{},"source":["Here's what the `test_model` looks like."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["[Row(meanfreq=0.0621823118609672, sd=0.0878894037873831, median=0.0109745762711864, Q25=0.00177966101694915, Q75=0.117457627118644, IQR=0.115677966101695, skew=9.61220808953177, kurt=114.803500510109, sp_ent=0.786650358576641, sfm=0.329569856469261, mode=0.000889830508474576, centroid=0.0621823118609672, meanfun=0.0997761550401998, minfun=0.0171122994652406, maxfun=0.258064516129032, meandom=0.0955528846153846, mindom=0.0078125, maxdom=1.4140625, dfrange=1.40625, modindx=0.105777777777778, label=1, features=DenseVector([0.0622, 0.0879, 0.011, 0.0018, 0.1175, 0.1157, 9.6122, 114.8035, 0.7867, 0.3296, 0.0009, 0.0622, 0.0998, 0.0171, 0.2581, 0.0956, 0.0078, 1.4141, 1.4062, 0.1058]), rawPrediction=DenseVector([-1.1187, 1.1187]), probability=DenseVector([0.2463, 0.7537]), prediction=1.0)]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["test_model.take(1)"]},{"cell_type":"markdown","metadata":{},"source":["## c. Model performance"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["areaUnderROC:  0.9877969706401062\n","areaUnderPR:  0.9879752286972516\n"]}],"source":["import pyspark.ml.evaluation as ev\n","\n","evaluator = ev.BinaryClassificationEvaluator(\n","    rawPredictionCol='probability', \n","    labelCol='label')\n","\n","print('areaUnderROC: ', evaluator.evaluate(test_model, \n","     {evaluator.metricName: 'areaUnderROC'}))\n","print('areaUnderPR: ', evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderPR'}))"]},{"cell_type":"markdown","metadata":{},"source":["## d. Saving the model"]},{"cell_type":"markdown","metadata":{},"source":["PySpark allows you to save the `Pipeline` definition for later use."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["pipelinePath = './voice'\n","pipeline.write().overwrite().save(pipelinePath)"]},{"cell_type":"markdown","metadata":{},"source":["So, you can load it up later and use straight away to `.fit(...)` and predict."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["[Row(meanfreq=0.0621823118609672, sd=0.0878894037873831, median=0.0109745762711864, Q25=0.00177966101694915, Q75=0.117457627118644, IQR=0.115677966101695, skew=9.61220808953177, kurt=114.803500510109, sp_ent=0.786650358576641, sfm=0.329569856469261, mode=0.000889830508474576, centroid=0.0621823118609672, meanfun=0.0997761550401998, minfun=0.0171122994652406, maxfun=0.258064516129032, meandom=0.0955528846153846, mindom=0.0078125, maxdom=1.4140625, dfrange=1.40625, modindx=0.105777777777778, label=1, features=DenseVector([0.0622, 0.0879, 0.011, 0.0018, 0.1175, 0.1157, 9.6122, 114.8035, 0.7867, 0.3296, 0.0009, 0.0622, 0.0998, 0.0171, 0.2581, 0.0956, 0.0078, 1.4141, 1.4062, 0.1058]), rawPrediction=DenseVector([-1.1187, 1.1187]), probability=DenseVector([0.2463, 0.7537]), prediction=1.0)]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["loadedPipeline = Pipeline.load(pipelinePath)\n","loadedPipeline \\\n","    .fit(voice_train)\\\n","    .transform(voice_test)\\\n","    .take(1)"]},{"cell_type":"markdown","metadata":{},"source":["You can also save the whole model and load it from disk."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from pyspark.ml import PipelineModel\n","\n","modelPath = './voice_model'\n","model.write().overwrite().save(modelPath)\n","\n","loadedPipelineModel = PipelineModel.load(modelPath)\n","test_loadedModel = loadedPipelineModel.transform(voice_test)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Hyper-parameter tuning"]},{"cell_type":"markdown","metadata":{},"source":["## a. Grid search"]},{"cell_type":"markdown","metadata":{},"source":["Now, we would like to figure out what training parameters work well. To this end, we specify our model and the list of parameters we want to loop through. Specifically, we test different maximum numbers of iterations and regularistaion parameters (L2)."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import pyspark.ml.tuning as tune\n","\n","logistic = cl.LogisticRegression(\n","    labelCol='label')\n","\n","grid = tune.ParamGridBuilder() \\\n","    .addGrid(logistic.maxIter,  \n","             [2, 10, 50]) \\\n","    .addGrid(logistic.regParam, \n","             [0.01, 0.05, 0.3]) \\\n","    .build()"]},{"cell_type":"markdown","metadata":{},"source":["Next, we need some way of comparing the models."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["evaluator = ev.BinaryClassificationEvaluator(\n","    rawPredictionCol='probability', \n","    labelCol='label')"]},{"cell_type":"markdown","metadata":{},"source":["Create the logic that will do the validation work for us."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["cv = tune.CrossValidator(\n","    estimator=logistic, \n","    estimatorParamMaps=grid, \n","    evaluator=evaluator\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Create a purely transforming `Pipeline`."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["pipeline = Pipeline(stages=[featuresCreator])\n","data_transformer = pipeline.fit(voice_train)"]},{"cell_type":"markdown","metadata":{},"source":["Having done this, we are ready to find the optimal combination of parameters for our model."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["cvModel = cv.fit(data_transformer.transform(voice_train))"]},{"cell_type":"markdown","metadata":{},"source":["The `cvModel` will return the best model estimated."]},{"cell_type":"markdown","metadata":{},"source":["## b. Model evaluation\n","\n","We can now use it to see if it performed better than our previous model."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["areaUnderROC 0.9916744475568025\n","areaUnderPR 0.9929399234828469\n"]}],"source":["data_train = data_transformer \\\n","    .transform(voice_test)\n","results = cvModel.transform(data_train)\n","\n","print('areaUnderROC', evaluator.evaluate(results, \n","     {evaluator.metricName: 'areaUnderROC'}))\n","print('areaUnderPR', evaluator.evaluate(results, \n","     {evaluator.metricName: 'areaUnderPR'}))"]},{"cell_type":"markdown","metadata":{},"source":["## c. Extract the parameters"]},{"cell_type":"markdown","metadata":{},"source":["What parameters has the best model? The answer is a little bit convoluted but here's how you can extract it."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["([{'maxIter': 50}, {'regParam': 0.01}], 0.9942463252028225)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["results = [\n","    (\n","        [\n","            {key.name: paramValue} \n","            for key, paramValue \n","            in zip(\n","                params.keys(), \n","                params.values())\n","        ], metric\n","    ) \n","    for params, metric \n","    in zip(\n","        cvModel.getEstimatorParamMaps(), \n","        cvModel.avgMetrics\n","    )\n","]\n","\n","sorted(results, \n","       key=lambda el: el[1], \n","       reverse=True)[0]"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Can we fit a more sophisticated model?\n","This demonstrates how to re-assemble the pipeline. We are using Gradient Boosted Trees here."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["areaUnderROC:  0.9932133347166023\n","areaUnderPR:   0.9930471486509133\n"]}],"source":["from pyspark.ml.classification import GBTClassifier\n","\n","# new classifier\n","gbt = GBTClassifier(\n","    labelCol='label', maxIter=20)\n","\n","# put into pipeline\n","pipelineGBT = Pipeline(stages=[\n","        featuresCreator, \n","        gbt\n","    ])\n","\n","# train and test\n","modelGBT = pipelineGBT.fit(voice_train)\n","test_modelGBT = modelGBT.transform(voice_test)\n","\n","# tell me the performance\n","evaluator = ev.BinaryClassificationEvaluator(\n","    rawPredictionCol='probability', \n","    labelCol='label')\n","\n","print('areaUnderROC: ', evaluator.evaluate(test_modelGBT, \n","     {evaluator.metricName: 'areaUnderROC'}))\n","print('areaUnderPR:  ', evaluator.evaluate(test_modelGBT, {evaluator.metricName: 'areaUnderPR'}))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["[Row(meanfreq=0.0621823118609672, sd=0.0878894037873831, median=0.0109745762711864, Q25=0.00177966101694915, Q75=0.117457627118644, IQR=0.115677966101695, skew=9.61220808953177, kurt=114.803500510109, sp_ent=0.786650358576641, sfm=0.329569856469261, mode=0.000889830508474576, centroid=0.0621823118609672, meanfun=0.0997761550401998, minfun=0.0171122994652406, maxfun=0.258064516129032, meandom=0.0955528846153846, mindom=0.0078125, maxdom=1.4140625, dfrange=1.40625, modindx=0.105777777777778, label=1, features=DenseVector([0.0622, 0.0879, 0.011, 0.0018, 0.1175, 0.1157, 9.6122, 114.8035, 0.7867, 0.3296, 0.0009, 0.0622, 0.0998, 0.0171, 0.2581, 0.0956, 0.0078, 1.4141, 1.4062, 0.1058]), rawPrediction=DenseVector([-1.0799, 1.0799]), probability=DenseVector([0.1034, 0.8966]), prediction=1.0)]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["test_modelGBT.take(1)"]},{"cell_type":"markdown","metadata":{},"source":["The performance looks similar."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}