{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "## Seminar class 7: Scalable machine learning I\n",
    "\n",
    "In this notebook we provide a simple implementation of batch gradient descent for logistic regression.  This implementation is not what you would use in practice, but it helps to understand the underlying concepts.\n",
    "\n",
    "We also demonstrate how the number of iterations affects the training error.\n",
    "\n",
    "We start by using the sample data set `sample_svm_data.txt`. \n",
    "\n",
    "Reference: https://github.com/apache/spark/blob/master/examples/src/main/python/logistic_regression.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "Recall that to train the logistic regression model, we will need the gradient of the losss function with respect to the weights $\\vec{w}$. The gradient will have entries \n",
    "$$\\frac{\\partial}{\\partial w_i} f(\\vec{w},\\text{data}),$$ \n",
    "where the loss function $f$ for given data is the negative log-likelihood $f(\\vec{w},\\text{data}) = -\\ell(\\vec{w})$. Our data consists of features $\\vec{x}_i$ and labels $y_i$ with $i = 1,\\dots, m$.\n",
    "\n",
    "For logistic regression, we know from the lecture that the log-likelihood is\n",
    "$$\n",
    "\\ell(\\vec{w}) = \\frac{1}{m}\\sum_{i=1}^m \\left[y_i \\vec{x}_i \\cdot \\vec{w} - \\log\\left(1+e^{\\vec{x}_i \\cdot \\vec{w}}\\right)\\right].\n",
    "$$\n",
    "Hence, our job is to implement the gradient (here written as a row vector)\n",
    "$$\n",
    "\\nabla \\ell(\\vec{w}) = \\frac{1}{m} \\sum_i \\left(y_i \\vec{x}_i - \\frac{e^{\\vec{x}_i \\cdot \\vec{w}}}{1+e^{\\vec{x}_i \\cdot \\vec{w}}} \\vec{x}_i\\right) \\\\ = \\sum_i \\frac{1}{m} \\vec{x}_i \\left( y_i - \\frac{1}{1+e^{-\\vec{x}_i \\cdot \\vec{w}}}\\right)\n",
    "$$\n",
    "such that we can perform steps towards the minimum of the loss function\n",
    "$$\n",
    "\\vec{w}_{t+1} = \\vec{w}_t -  \\eta_t\\nabla f(\\vec{w}_t,\\text{data}) \\\\\n",
    "= \\vec{w}_t + \\frac{\\eta_t}{m}  \\left[  \\sum_i \\vec{x}_i \\left( y_i - \\frac{1}{1+e^{-\\vec{x}_i \\cdot \\vec{w}}}\\right) \\right] \n",
    "$$\n",
    "As we can see below, we will store the data in numpy matrices, such that this gradient can be computed  entirely with numpy functions.\n",
    "\n",
    "Instead of computing this sum for all indices $i$ at once, we can split the training data into, say, 5 batches $B_j$ of size $n = m/5$. Also, we will convert everything to row vectors.\n",
    "This means, we will get\n",
    "$$\n",
    "\\vec{w}_{t+1}^\\top = \\vec{w}_t^\\top - \\frac{\\eta_t}{5}  \\sum_{j=1}^5\\left\\{ \\frac{1}{n} \\sum_{i\\in B_j} \\vec{x}^\\top_i \\left( y_i - \\frac{1}{1+e^{-\\vec{x}_i \\cdot \\vec{w}}}\\right) \\right\\} \\quad (\\star)\n",
    "$$\n",
    "Note that we really did nothing here!\n",
    "\n",
    "Now, the term in $\\{\\}$ can be computed by separate workers on separate partitions of an RDD.\n",
    "This is a map step.\n",
    "\n",
    "Then, we can perform the outer sum as reduce step, which requires workers to talk to the master.\n",
    "\n",
    "Next, we update the weight vector $\\vec{w}_{t} \\to \\vec{w}_{t+1} = \\vec{w}_{t} +\\eta_t (\\dots)$. Because the weight vector is small (here is has length 16), we can easily do this on the master node.\n",
    "\n",
    "Please sure you understand where the different parts of $(\\star)$ show up in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to where Spark lives on your machine/compute engine (usually $SPARK_HOME)\n",
    "spark_home = \"file:///usr/lib/spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The key idea is that we will load the data into an RDD and use the RDD's partitioning as our batches.\n",
    "We then use `mapPartitions` to apply a partition-wise function to format the data as a numpy array in the right shape. This will allow us to call optimised (i.e. fast) numpy functions for each partition.\n",
    "\n",
    "![GD](./fig/distributedGD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "D = 16             # Number of dimensions (size of weight vector)\n",
    "iterations = 200   # Number of iterations\n",
    "\n",
    "# Read a batch of points from the input file into a NumPy matrix object. We operate on batches to\n",
    "# make further computations faster.\n",
    "# The data file contains lines of the form <label> <x1> <x2> ... <xD>. We load each block of these\n",
    "# into a NumPy array of size numLines * (D + 1) and pull out column 0 vs the others in gradient().\n",
    "def readPointBatch(iterator):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i, s in enumerate(strs):\n",
    "        matrix[i] = np.fromstring(s, dtype=np.float32, sep=' ')\n",
    "    return [matrix]\n",
    "\n",
    "# dataset to be used\n",
    "# here we will be using the partitions as batches.\n",
    "points = spark.read.text(spark_home+\"/data/mllib/sample_svm_data.txt\").rdd.repartition(5)\n",
    "\n",
    "# number of data points\n",
    "n = points.count() \n",
    "\n",
    "# see explanation in the next cell\n",
    "points = points.map(lambda r: r[0])\\\n",
    "    .mapPartitions(readPointBatch).cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mapPartition` operates on individual partitions of the RDD, that is, the `for` loop inside `readPointBatch` will iterate though the elements of each partition.\n",
    "Here, caching the data prevents future re-loading of the data upon each lazy evaluation. This is important, as we will later use it in each iteration of the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial w: [ 0.80447849  0.58412445  0.88865674 -0.57983304 -0.13572482  0.1102371\n",
      "  0.97465562 -0.9534808  -0.69847927 -0.63210783  0.39723133 -0.34048155\n",
      " -0.40044425  0.27824074  0.18882086 -0.73738064]\n",
      "*** On iteration 50 ***\n",
      "Gradient descent:  [ 0.40984691  0.30165578  0.39996973 -0.38815515 -0.00945966  0.03082005\n",
      "  0.35824087 -0.29805962 -0.21545777 -0.21545777  0.0241376  -0.07573616\n",
      " -0.32667583  0.06440437 -0.03940016 -0.29728247]\n",
      "Current w:  [ 0.24396379  0.21258616  0.30056597 -0.08571358 -0.23439455 -0.06501694\n",
      "  0.38614501 -0.69331102 -0.12742544 -0.061054    0.4295895  -0.34451543\n",
      " -0.02610691  0.19775516  0.23076987 -0.4380877 ]\n",
      "Total error: 0.4409937888198758\n",
      "*** On iteration 100 ***\n",
      "Gradient descent:  [ 0.15704631  0.09376574  0.11783044 -0.15949301 -0.03229026 -0.02056571\n",
      "  0.08312746 -0.13214796 -0.02908986 -0.02908986  0.03388195 -0.06624602\n",
      " -0.09886752  0.11665199  0.09070388 -0.10750354]\n",
      "Current w:  [-0.0188042   0.02890333  0.0654804   0.17865693 -0.20304101 -0.05520608\n",
      "  0.19366239 -0.48248681 -0.03563994  0.0307315   0.39573501 -0.262904\n",
      "  0.17129997  0.10291042  0.20394604 -0.23485383]\n",
      "Total error: 0.40062111801242234\n",
      "*** On iteration 150 ***\n",
      "Gradient descent:  [ 0.0633365   0.02183904  0.02918551 -0.05346791 -0.01576963 -0.00848399\n",
      "  0.01022804 -0.05487935 -0.0058789  -0.0058789   0.02437373 -0.02842501\n",
      " -0.06158944  0.07476277  0.08852708 -0.07414759]\n",
      "Current w:  [-0.12017299 -0.02032683  0.00249225  0.27488059 -0.17958834 -0.04084202\n",
      "  0.15634028 -0.39577971 -0.02232769  0.04404375  0.36667054 -0.21793732\n",
      "  0.24682394  0.00841181  0.10982511 -0.14847733]\n",
      "Total error: 0.37888198757763975\n",
      "*** Final w: [-0.1629251  -0.03141367 -0.01244786  0.30711783 -0.16906283 -0.03590079\n",
      "  0.15612523 -0.35825692 -0.01849016  0.04788128  0.34654977 -0.19961561\n",
      "  0.29963299 -0.05302986  0.03376405 -0.08297327]***\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize w to a random values in [-1,1].\n",
    "w = 2 * np.random.ranf(size=D) - 1\n",
    "print(\"Initial w: \" + str(w))\n",
    "\n",
    "# Compute logistic regression gradient for a matrix of data points\n",
    "def gradient(matrix, w):\n",
    "    Y = matrix[:, 0]    # point labels (first column of input file)\n",
    "    X = matrix[:, 1:]   # point coordinates\n",
    "    \n",
    "    # For each point (x, y), compute gradient function \n",
    "    # (see the lecture note \"gradient descent for logistic regression\")\n",
    "    # and take the average of it\n",
    "    # note the minus sign is needed because minimizing the loss function is equal to \n",
    "    # minimizing the negative log likelihood\n",
    "    return -(X.T * (Y - 1.0/(1.0+np.exp(-X.dot(w))))).sum(1) / len(Y)\n",
    "\n",
    "def add(x, y):\n",
    "    x += y\n",
    "    return x\n",
    "\n",
    "# calculate classification error\n",
    "def pred_err(matrix, w):\n",
    "    Y = matrix[:, 0]\n",
    "    X = matrix[:, 1:]\n",
    "    # this is the number of miss-classified examples.\n",
    "    err = sum(Y!=(1.0/(1.0+np.exp(-X.dot(w)))>0.5))  \n",
    "    return (err)\n",
    "\n",
    "# train the model using gradient descent with step size eta\n",
    "eta = 0.1\n",
    "err = np.zeros(iterations)\n",
    "\n",
    "for i in range(iterations):\n",
    "    g = points.map(lambda m: gradient(m, w)) # compute batch gradient at point w\n",
    "    num_batch = g.getNumPartitions()\n",
    "    g = g.reduce(add)                        # compute sum of batch gradients \n",
    "    # use eta as the step size\n",
    "    w -= eta/num_batch * g                   # convert sum to average gradient and perform gradient descent step \n",
    "    err[i] = points.map(lambda m: pred_err(m, w)).reduce(add)/float(n)\n",
    "    \n",
    "    if (i + 1) % 50 == 0 and i+1 != iterations:\n",
    "        print(\"*** On iteration %i ***\" % (i + 1))\n",
    "        print(\"Gradient descent: \", str(g))\n",
    "        print(\"Current w: \", str(w))\n",
    "        print(\"Total error:\", err[i])\n",
    "\n",
    "print(\"*** Final w: \" + str(w) + \"***\")\n",
    "\n",
    "# plot the training error over time\n",
    "plt.plot(err)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Trainning error\")\n",
    "plt.xticks(np.arange(0, iterations+1, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Further discussions and activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Termination criterion\n",
    "\n",
    "In the optimisation above, we fix the number of iterations of gradient descent in advance. However, choosing the right number of iterations is tricky: fewer iterations leads to underfitting and more iterations will take more time.\n",
    "Instead, people use termination criterion to stop the optimisation.\n",
    "We will specify a precision parameter $\\epsilon > 0$ and run $T(\\epsilon)$ rounds:\n",
    "\n",
    "$$\n",
    "T(\\epsilon) = \\min \\{t\\geq 1: ||\\vec{w}_{t} - \\vec{w}_{t-1}||_\\infty \\leq \\epsilon\\}\n",
    "$$\n",
    "\n",
    "Can you plot $T(\\epsilon)$ as a function of $\\epsilon$? \n",
    "\n",
    "(hint: log-log plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation speedup from data parallelism\n",
    "\n",
    "Dividing the data into batches and run the computation on batches in parallel is called [data parallelism](https://en.wikipedia.org/wiki/Data_parallelism).\n",
    "Choosing the right number of batches can be also tricky: more batches lead to smaller matrix size in each batches and as a result we might lose the speedup from the highly optimised linear algebra library, e.g. `numpy`\n",
    "\n",
    "Can you plot the computation time as a function of the number of batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real applications: Criteo CTR prediction challenge\n",
    "\n",
    "In the lecture, we introduced the [Criteo Ads click dataset](https://labs.criteo.com/2013/12/download-terabyte-click-logs/). \n",
    "\n",
    "Can you train a logistic regression model to predict the CTR using the Day 0 data from Criteo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
